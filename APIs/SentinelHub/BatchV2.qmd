---
title: Batch Processing V2 API
---

**The BatchV2 API is only available for users with Copernicus Service
accounts.** Please refer to our [FAQ](/FAQ.qmd) on account typology
change and [Submit A
Request](https://helpcenter.dataspace.copernicus.eu/hc/en-gb/requests/new){target="_blank"}
to our Copernicus Data Space Ecosystem Support Team to request your
Copernicus Service account accordingly.

------------------------------------------------------------------------

## Migration guide

If you\'re interested on how to migrate from [Batch Processing
API](/APIs/SentinelHub/Batch.qmd) to **BatchV2**, please read the
following guide:

[BatchV2 Migration Guide](/APIs/SentinelHub/BatchV2/Migration.qmd)

------------------------------------------------------------------------

## Overview

**BatchV2 Processing API** (or shortly \"**BatchV2 API**\") enables you
to request data for large areas and/or longer time periods for any
Sentinel Hub supported collection, including BYOC (bring your own data).
It is an asynchronous REST service, meaning data won\'t be returned
immediately but delivered to your specified object storage instead.

### Workflow

The Batch V2 Processing API comes with the set of REST APIs which
support the execution of various workflows. The diagram below shows all
possible statuses of a batch task:

-   `CREATED`
-   `ANALYSING`
-   `ANALYSIS_DONE`
-   `PROCESSING`
-   `DONE`
-   `FAILED`
-   `STOPPED`

and user\'s actions:

-   `ANALYSE`
-   `START`
-   `STOP`

which trigger transitions among them.

``` {mermaid}
stateDiagram
    [*]-->CREATED
    CREATED-->ANALYSING: #128100; START/ANALYSE
    state fork_state_analysis <<fork>>
    ANALYSING --> fork_state_analysis
    fork_state_analysis --> FAILED
    fork_state_analysis --> ANALYSIS_DONE
    state fork_state_analysis_done <<fork>>
    ANALYSIS_DONE-->fork_state_analysis_done
    fork_state_analysis_done-->PROCESSING: #128100; START
    PROCESSING--> STOPPED: #128100; STOP
    fork_state_analysis_done--> STOPPED: #128100; STOP
    PROCESSING--> DONE
    STOPPED --> ANALYSIS_DONE: #128100; START
    DONE-->[*]
    FAILED-->[*]
```

The workflow starts when a user posts a new batch request. In this step
the system:

-   creates a new batch task with the status `CREATED`,
-   validates the user\'s input (except the evalscript),
-   ensures the user\'s account has at least 1000 PUs,
-   uploads a JSON of the original request to the user\'s bucket,
-   and returns the overview of the created task.

The user can then decide to either request an additional analysis of the
task or start the processing. When an additional analysis is requested:

-   the status of the task changes to `ANALYSING`,
-   the evalscript is validated,
-   a [feature manifest](/APIs/SentinelHub/BatchV2.qmd#feature-manifest)
    file is uploaded to the user\'s bucket,
-   after the analysis is finished, the status of the task changes to
    `ANALYSIS_DONE`.

If the user chooses to directly start processing, the system still
executes the analysis but when the analysis is done it automatically
proceeds with processing. This is not explicitly shown in the diagram in
order to keep it simple.

When the user starts the processing:

-   the status of the task changes to `PROCESSING` (this may take a
    while, depending on the load on the service),
-   the processing starts,
-   an [execution
    database](/APIs/SentinelHub/BatchV2.qmd#execution-database) is
    periodically uploaded to the user\'s bucket,
-   spent processing units are billed periodically.

When the processing is finished, the status of the task changes to
`DONE`.

#### Stopping the request

A task might be stopped for the following reasons:

-   it\'s requested by a user (user action),
-   user is out of processing units,
-   something is wrong with the processing of the task (e.g. the system
    is not able to process the data).

A user may stop the request in following states: `ANALYSING`,
`ANALYSIS_DONE` and `PROCESSING`. However:

-   if the status is `ANALYSING`, the analysis will complete,
-   if the status is `PROCESSING`, all features (polygons) that have
    been processed or are being processed at that moment are charged
    for,
-   user is not allowed to restart the task in the next 30 minutes.

------------------------------------------------------------------------

## Input features

BatchV2 API supports two ways of specifying the input features of your
batch task:

1.  Pre-defined [Tiling
    Grid](/APIs/SentinelHub/BatchV2.qmd#1-tiling-grid)
2.  User-defined
    [GeoPackage](/APIs/SentinelHub/BatchV2.qmd#2-geopackage)

### 1. Tiling grid {#1-tiling-grid}

For more effective processing we divide the area of interest into tiles
and process each tile separately. While `process` API uses grids which
come together with each datasource for processing of the data, the
`batch` API uses one of the predefined tiling grids. The predefined
tiling grids 0-2 are based on the [Sentinel-2
tiling](https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products){target="_blank"}
in WGS84/UTM projection with some adjustments:

-   The width and height of tiles in the original Sentinel 2 grid is 100
    km while the width and height of tiles in our grids are given in the
    table below.
-   All redundant tiles (i.e. fully overlapped tiles) are removed.

All available tiling grids can be requested with (*NOTE: To run this
example you need to first create an OAuth client as is explained
[here](/APIs/SentinelHub/Overview/Authentication.qmd#python)*):

``` python
url = "https://sh.dataspace.copernicus.eu/api/v2/batch/tilinggrids/"

response = oauth.request("GET", url)

response.json()
```

This will return the list of available grids and information about tile
size and available resolutions for each grid. Currently, available grids
are:

  name                  id   tile size   resolutions                        coverage                                                                  output CRS   download the grid \[zip with shp file\] \*\*
  --------------------- ---- ----------- ---------------------------------- ------------------------------------------------------------------------- ------------ ----------------------------------------------------------------------------------------------------------------
  UTM 20km grid         0    20040 m     10 m, 20 m, 30m\*, 60 m            World, latitudes from -80.7° to 80.7°                                     UTM          [UTM 20km grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-0.zip){target="_blank"}
  UTM 10km grid         1    10000 m     10 m, 20 m                         World, latitudes from -80.6° to 80.6°                                     UTM          [UTM 10km grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-1.zip){target="_blank"}
  UTM 100km grid        2    100080 m    30m\*, 60 m, 120 m, 240 m, 360 m   World, latitudes from -81° to 81°                                         UTM          [UTM 100km grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-2.zip){target="_blank"}
  WGS84 1 degree grid   3    1 °         0.0001°, 0.0002°                   World, all latitudes                                                      WGS84        [WGS84 1 degree grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-3.zip){target="_blank"}
  LAEA 100km grid       6    100000 m    40 m, 50 m, 100 m                  Europe, including Turkey, Iceland, Svalbald, Azores, and Canary Islands   EPSG:3035    [LAEA 100km grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-6.zip){target="_blank"}
  LAEA 20km grid        7    20000 m     10 m, 20 m                         Europe, including Turkey, Iceland, Svalbald, Azores, and Canary Islands   EPSG:3035    [LAEA 20km grid](https://s3.eu-central-1.amazonaws.com/sh-batch-grids/tiling-grid-7.zip){target="_blank"}

\*\* The geometries of the tiles are reprojected to WGS84 for download.
Because of this and other reasons the geometries of the output rasters
may differ from the tile geometries provided here.

To use `20km` grid with 60 m resolution, for example, specify `id` and
`resolution` parameters of the `tilingGrid` object when creating a new
batch request (see an example of [full
request](/APIs/SentinelHub/BatchV2/Examples.qmd#create-a-batchv2-processing-request))
as:

``` JSON
{
  ...
  "tilingGrid": {
    "id": 0,
    "resolution": 60.0
  },
  ...
}
```

### 2. GeoPackage {#2-geopackage}

In addition to the predefined tiling grids, BatchV2 API now also support
user-defined features through
[GeoPackages](https://www.geopackage.org/spec/){target="_blank"}. This
allows you to specify features of any shape as long as the underlying
geometry is a POLYGON or MULTIPOLYGON in an **EPSG compliant** CRS
listed [here](/APIs/SentinelHub/Process/Crs.qmd). The GeoPackage can
also have multiple layers, offering more flexibility in specifying
features in multiple CRS.

The GeoPackage must adhere to the [GeoPackage
spec](https://www.geopackage.org/spec/){target="_blank"} and contain at
**least one feature table with any name**. The table must include a
column that holds the geometry data. This column can be named
arbitrarily, but it must be listed as the geometry column in the
`gpkg_geometry_columns` table. The table schema should include the
following columns:

  Column       Type                      Example
  ------------ ------------------------- ----------------------------------------------------------
  id           INTEGER **(UNIQUE)**      1000
  identifier   TEXT **(UNIQUE)**         FEATURE_NAME
  geometry     POLYGON or MULTIPOLYGON   Feature geometry representation in GeoPackage WKB format
  width        INTEGER                   1000
  height       INTEGER                   1000
  resolution   REAL                      0.005

#### Caveats

-   You must specify either both width and height, or alternatively,
    specify resolution. If both values are provided, width and height
    will be used, and resolution will be ignored.
-   The feature table must use a CRS that is **EPSG compliant**.
-   Both `id` and `identifier` values must not be null and unique across
    all feature tables.

An example of a batch task with GeoPackage input is available
[here](/APIs/SentinelHub/BatchV2/Examples.qmd#option-3-geopackage-input-and-geotiff-output).

------------------------------------------------------------------------

## Processing results

The outputs of a batch task will be stored to your object storage in
either:

1.  GeoTIFF (and JSON for metadata) or
2.  Zarr format

### 1. GeoTIFF output format {#1-geotiff-output-format}

**The GeoTIFF format will be used if your request includes the
`output.type` parameter set to `raster`, along with other relevant
parameters specified in the [BatchV2 API
reference](/APIs/SentinelHub/ApiReference.qmd#tag/batch_v2_process/operation/createNewBatchV2ProcessingRequest).
An example of a batch task with GeoTIFF output is available
[here](/APIs/SentinelHub/BatchV2/Examples.qmd#option-1-geotiff-format-output).**

By default, the results will be organized in sub-folders where one
sub-folder will be created for each feature. Each sub-folder might
contain one or more images depending on how many outputs were defined in
the [evalscript](/APIs/SentinelHub/Evalscript/V3.qmd#setup-function) of
the request. For example:

You can also customize the sub-folder structure and file naming as
described in the `delivery` parameter under `output` in [BatchV2 API
reference](/APIs/SentinelHub/ApiReference.qmd#tag/batch_v2_process/operation/createNewBatchV2ProcessingRequest).

You can choose to return your GeoTIFF files as Cloud Optimized GeoTIFF
(COG), by setting the `cogOutput` parameter under `output` in your
request as `true`. Several advanced COG options can be selected as
well - read about the parameter in [BatchV2 API
reference](/APIs/SentinelHub/ApiReference.qmd#tag/batch_v2_process/operation/createNewBatchV2ProcessingRequest).

The output projection depends on the selected input, either tiling grid
or GeoPackage:

1.  If the input is a tiling grid, the results of batch processing will
    be in the projection of the selected [tiling
    grid](/APIs/SentinelHub/BatchV2.qmd#1-tiling-grid). For UTM-based
    grids, each part of the AOI (area of interest) is delivered in the
    UTM zone with which it intersects. In other words, in case your AOI
    intersects with more UTM zones, the results will be delivered as
    tiles in different UTM zones (and thus different CRSs).
2.  If the input is a GeoPackage, the results will be in the same CRS as
    the input feature\'s CRS.

### 2. Zarr output format {#2-zarr-output-format}

The Zarr format will be used if your request includes the `output.type`
parameter set to `zarr`, along with other relevant parameters specified
in the [BatchV2 API
reference](/APIs/SentinelHub/ApiReference.qmd#tag/batch_v2_process/operation/createNewBatchV2ProcessingRequest).
An example of a batch request with Zarr output is available
[here](/APIs/SentinelHub/BatchV2/Examples.qmd#option-2-zarr-format-output).
Your request **must** only have one band per output and the
`application/json` format in responses is **not** supported.

The outputs of batch processing will be stored as a single Zarr group
containing one data array for each evalscript output and multiple
coordinate arrays. The output will be stored in a subfolder named after
the `requestId` that you pass to the API in the `delivery.s3.url`
parameter under `output`.

------------------------------------------------------------------------

## Ingesting results into BYOC

#### Purpose

Enables automatic ingestion of processing results into a BYOC
collection, allowing you to:

-   Access data with Processing API, by using the collection ID
-   Create a configuration with custom layers
-   Make OGC requests to a configuration
-   View data in EO Browser

In order to enable this functionality, user needs to specify either id
of an existing BYOC collection (`collectionId`) or set
`createCollection = true`.

``` JSON
{
  ...
  "output": {
    ...
    "createCollection": true,
    "collectionId": "<byoc-collection-id>",
    ...
  },
  ...
}
```

If collectionId is provided, the existing collection will be used for
data ingestion.

If `createCollection` is set to `true` and `collectionId` is not
provided, a new BYOC collection will be created automatically and the
collection bands will be set according to the request output `responses`
definitions.

Regardless of whether the user specifies an existing collection or
requests a new one, processed data will still be uploaded to the users
bucket, where they will be available for download and analysis.

When creating a new batch collection, one has to be careful to:

-   Make sure that `cogOutput=true` and that the output format is a
    `image/tiff`
-   If an existing BYOC collection is used, make sure that `identifier`
    and `sampleType` from the output definition(s) match the name and
    the type of the BYOC band(s). Single band and multi-band outputs are
    supported.
-   If multi-band output is used in the request, the additionally
    generated bands will be named using a numerical suffix in ascending
    order (e.g. 2, \... 99). For example, if the
    `output: { id: "result", bands: 3 }` is used in the evalscript setup
    function, the produced BYOC bands will be named: `result` for band
    1, `result2` for band 2 and `result3` for band 3. Make sure that no
    other output band has any of these automatically generated names, as
    this will throw an error during the analysis phase. The
    `output: [{ id: "result", bands: 3 },{ id: "result2", bands: 1 }]`
    will throw an exception.
-   Keep sampleType in mind, as the values the evalscript returns when
    creating a collection will be the values available when making a
    request to access it.

## Feature Manifest

#### Purpose

-   Provides a detailed overview of features scheduled for processing
    during the `PROCESSING` step.
-   Enables users to verify feature information and corresponding output
    paths prior to processing.

#### Key Information

-   **File Type:**
    [GeoPackage](https://www.geopackage.org/spec/){target="_blank"}
-   **File Name:** `featureManifest-<requestId>.gpkg`
-   **Location:** Root folder of the specified output delivery path
-   **Structure:**
    -   May contain multiple feature tables, one per distinct CRS used
        by the features.
    -   Table names follow the format `feature_<crs-id>` (e.g.
        `feature_4326`).

During task analysis, the system will upload a file to the user\'s
bucket called the `featureManifest-<requestId>.gpkg`. This file is a
GeoPackage that contains basic information about the features that will
be processed during the `PROCESSING` step. It is intended to be used by
users to check the features that will be processed and their
corresponding output paths.

If the output type is set to `raster`, the output paths will be the
paths to the GeoTIFF files. If the output type is `zarr`, the output
paths will just be the root of the output folder.

The database may contain multiple feature tables, one feature table for
each CRS of all features. The tables will be named `feature_<crs-id>`,
e.g. `feature_4326`. `<br/>`{=html}The schema of feature tables inside
the database is currently the following:

  Name         Type       Description
  ------------ ---------- -------------------------------------------------------------------
  fid          INTEGER    Auto-incrementing ID
  id           INTEGER    Numerical ID of the feature
  identifier   TEXT       Textual ID of the feature
  path         TEXT       The object storage path URI where the feature will be uploaded to
  geometry     GEOMETRY   Feature geometry representation in GeoPackage WKB format

------------------------------------------------------------------------

## Execution database

### Purpose

The Execution Database serves as a monitoring tool for tracking the
progress of feature execution within a specific task. It provides users
with insight into the status of each feature being processed.

### Key Information

-   **File Type:** SQLite
-   **File Name:** `execution-<requestId>.sqlite`
-   **Location:** Root folder of specified output delivery path
-   **Structure:**
    -   Contains a single table called `features`.

You can monitor the execution of your features for a specific task by
checking the SQLite database that is uploaded to your bucket. The
database contains the name and status of each feature. The database is
updated periodically during the execution of the task.

The database can be found in your bucket in the root output folder and
is named `execution-<requestId>.sqlite`.

The schema of the `features` table is currently the following:

  Name        Type      Description
  ----------- --------- ------------------------------------------------------------------
  id          INTEGER   Numerical ID of the feature
  name        TEXT      Textual ID of the feature
  status      TEXT      Status of the feature (PENDING, DONE, FAILED, etc.)
  error       TEXT      Error message in case processing has failed
  delivered   BOOLEAN   `True` if output delivered to delivery bucket, otherwise `False`

------------------------------------------------------------------------

## Bucket settings and access

The results will be delivered in your own bucket hosted at Copernicus
Data Space Ecosystem. To access your bucket accessKey and
secretAccessKey pair have to bo provided in your request.

``` default
s3 = {
    "url": "s3://<your-bucket>/<path>",
    "accessKey": "<your-bucket-access-key>",
    "secretAccessKey": "<your-bucket-secret-access-key>"
}
```

If you do not yet have a bucket at Copernicus Data Space Ecosystem,
please follow [these
steps](https://creodias.docs.cloudferro.com/en/latest/s3/Create-S3-bucket-and-use-it-in-Sentinel-Hub-requests.html){target="_blank"}
to get one.

## Examples

[Example of Batch Processing
Workflow](/APIs/SentinelHub/BatchV2/Examples.qmd)
